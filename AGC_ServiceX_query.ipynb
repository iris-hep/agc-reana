{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "import awkward as ak\n",
    "import cabinetry\n",
    "import cloudpickle\n",
    "import correctionlib\n",
    "from coffea import processor\n",
    "from coffea.nanoevents import NanoAODSchema\n",
    "from coffea.analysis_tools import PackedSelection\n",
    "import copy\n",
    "#from func_adl import ObjectStream\n",
    "#from func_adl_servicex import ServiceXSourceUpROOT\n",
    "import hist\n",
    "import numpy as np\n",
    "import pyhf\n",
    "\n",
    "import sys\n",
    "import utils  # contains code for bookkeeping and cosmetics, as well as some boilerplate\n",
    "\n",
    "logging.getLogger(\"cabinetry\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GLOBAL CONFIGURATION\n",
    "# input files per process, set to e.g. 10 (smaller number = faster)\n",
    "N_FILES_MAX_PER_SAMPLE = 1\n",
    "\n",
    "# enable Dask\n",
    "USE_DASK = False\n",
    "\n",
    "# enable ServiceX\n",
    "USE_SERVICEX = True\n",
    "\n",
    "### ML-INFERENCE SETTINGS\n",
    "\n",
    "# enable ML inference\n",
    "USE_INFERENCE = False \n",
    "\n",
    "# enable inference using NVIDIA Triton server\n",
    "USE_TRITON = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TtbarAnalysis(processor.ProcessorABC):\n",
    "    def __init__(self, use_inference, use_triton):\n",
    "\n",
    "        # initialize dictionary of hists for signal and control region\n",
    "        self.hist_dict = {}\n",
    "        for region in [\"4j1b\", \"4j2b\"]:\n",
    "            self.hist_dict[region] = (\n",
    "                hist.Hist.new.Reg(utils.config[\"global\"][\"NUM_BINS\"], \n",
    "                                  utils.config[\"global\"][\"BIN_LOW\"], \n",
    "                                  utils.config[\"global\"][\"BIN_HIGH\"], \n",
    "                                  name=\"observable\", \n",
    "                                  label=\"observable [GeV]\")\n",
    "                .StrCat([], name=\"process\", label=\"Process\", growth=True)\n",
    "                .StrCat([], name=\"variation\", label=\"Systematic variation\", growth=True)\n",
    "                .Weight()\n",
    "            )\n",
    "        \n",
    "        self.cset = correctionlib.CorrectionSet.from_file(\"corrections.json\")\n",
    "        self.use_inference = use_inference\n",
    "        \n",
    "        # set up attributes only needed if USE_INFERENCE=True\n",
    "        if self.use_inference:\n",
    "            \n",
    "            # initialize dictionary of hists for ML observables\n",
    "            self.ml_hist_dict = {}\n",
    "            for i in range(len(utils.config[\"ml\"][\"FEATURE_NAMES\"])):\n",
    "                self.ml_hist_dict[utils.config[\"ml\"][\"FEATURE_NAMES\"][i]] = (\n",
    "                    hist.Hist.new.Reg(utils.config[\"global\"][\"NUM_BINS\"],\n",
    "                                      utils.config[\"ml\"][\"BIN_LOW\"][i],\n",
    "                                      utils.config[\"ml\"][\"BIN_HIGH\"][i],\n",
    "                                      name=\"observable\",\n",
    "                                      label=utils.config[\"ml\"][\"FEATURE_DESCRIPTIONS\"][i])\n",
    "                    .StrCat([], name=\"process\", label=\"Process\", growth=True)\n",
    "                    .StrCat([], name=\"variation\", label=\"Systematic variation\", growth=True)\n",
    "                    .Weight()\n",
    "                )\n",
    "            \n",
    "            self.use_triton = use_triton\n",
    "\n",
    "    def only_do_IO(self, events):\n",
    "        for branch in utils.config[\"benchmarking\"][\"IO_BRANCHES\"][\n",
    "            utils.config[\"benchmarking\"][\"IO_FILE_PERCENT\"]\n",
    "        ]:\n",
    "            if \"_\" in branch:\n",
    "                split = branch.split(\"_\")\n",
    "                object_type = split[0]\n",
    "                property_name = \"_\".join(split[1:])\n",
    "                ak.materialized(events[object_type][property_name])\n",
    "            else:\n",
    "                ak.materialized(events[branch])\n",
    "        return {\"hist\": {}}\n",
    "\n",
    "    def process(self, events):\n",
    "        if utils.config[\"benchmarking\"][\"DISABLE_PROCESSING\"]:\n",
    "            # IO testing with no subsequent processing\n",
    "            return self.only_do_IO(events)\n",
    "\n",
    "        # create copies of histogram objects\n",
    "        hist_dict = copy.deepcopy(self.hist_dict)\n",
    "        if self.use_inference:\n",
    "            ml_hist_dict = copy.deepcopy(self.ml_hist_dict)\n",
    "\n",
    "        process = events.metadata[\"process\"]  # \"ttbar\" etc.\n",
    "        variation = events.metadata[\"variation\"]  # \"nominal\" etc.\n",
    "\n",
    "        # normalization for MC\n",
    "        x_sec = events.metadata[\"xsec\"]\n",
    "        nevts_total = events.metadata[\"nevts\"]\n",
    "        lumi = 3378 # /pb\n",
    "        if process != \"data\":\n",
    "            xsec_weight = x_sec * lumi / nevts_total\n",
    "        else:\n",
    "            xsec_weight = 1\n",
    "\n",
    "        # setup triton gRPC client\n",
    "        if self.use_inference and self.use_triton:\n",
    "            triton_client = utils.clients.get_triton_client(utils.config[\"ml\"][\"TRITON_URL\"])\n",
    "\n",
    "\n",
    "        #### systematics\n",
    "        # jet energy scale / resolution systematics\n",
    "        # need to adjust schema to instead use coffea add_systematic feature, especially for ServiceX\n",
    "        # cannot attach pT variations to events.jet, so attach to events directly\n",
    "        # and subsequently scale pT by these scale factors\n",
    "        events[\"pt_scale_up\"] = 1.03\n",
    "        events[\"pt_res_up\"] = utils.systematics.jet_pt_resolution(events.Jet.pt)\n",
    "\n",
    "        syst_variations = [\"nominal\"]\n",
    "        jet_kinematic_systs = [\"pt_scale_up\", \"pt_res_up\"]\n",
    "        event_systs = [f\"btag_var_{i}\" for i in range(4)]\n",
    "        if process == \"wjets\":\n",
    "            event_systs.append(\"scale_var\")\n",
    "\n",
    "        # Only do systematics for nominal samples, e.g. ttbar__nominal\n",
    "        if variation == \"nominal\":\n",
    "            syst_variations.extend(jet_kinematic_systs)\n",
    "            syst_variations.extend(event_systs)\n",
    "\n",
    "        # for pt_var in pt_variations:\n",
    "        for syst_var in syst_variations:\n",
    "            ### event selection\n",
    "            # very very loosely based on https://arxiv.org/abs/2006.13076\n",
    "\n",
    "            # Note: This creates new objects, distinct from those in the 'events' object\n",
    "            elecs = events.Electron\n",
    "            muons = events.Muon\n",
    "            jets = events.Jet\n",
    "            if syst_var in jet_kinematic_systs:\n",
    "                # Replace jet.pt with the adjusted values\n",
    "                jets[\"pt\"] = jets.pt * events[syst_var]\n",
    "\n",
    "            electron_reqs = (elecs.pt > 30) & (np.abs(elecs.eta) < 2.1) & (elecs.cutBased == 4) & (elecs.sip3d < 4)\n",
    "            muon_reqs = ((muons.pt > 30) & (np.abs(muons.eta) < 2.1) & (muons.tightId) & (muons.sip3d < 4) &\n",
    "                         (muons.pfRelIso04_all < 0.15))\n",
    "            jet_reqs = (jets.pt > 30) & (np.abs(jets.eta) < 2.4) & (jets.isTightLeptonVeto)\n",
    "\n",
    "            # Only keep objects that pass our requirements\n",
    "            elecs = elecs[electron_reqs]\n",
    "            muons = muons[muon_reqs]\n",
    "            jets = jets[jet_reqs]\n",
    "\n",
    "            if self.use_inference:\n",
    "                even = (events.event%2==0)  # whether events are even/odd\n",
    "\n",
    "            B_TAG_THRESHOLD = 0.5\n",
    "\n",
    "            ######### Store boolean masks with PackedSelection ##########\n",
    "            selections = PackedSelection(dtype='uint64')\n",
    "            # Basic selection criteria\n",
    "            selections.add(\"exactly_1l\", (ak.num(elecs) + ak.num(muons)) == 1)\n",
    "            selections.add(\"atleast_4j\", ak.num(jets) >= 4)\n",
    "            selections.add(\"exactly_1b\", ak.sum(jets.btagCSVV2 > B_TAG_THRESHOLD, axis=1) == 1)\n",
    "            selections.add(\"atleast_2b\", ak.sum(jets.btagCSVV2 > B_TAG_THRESHOLD, axis=1) >= 2)\n",
    "            # Complex selection criteria\n",
    "            selections.add(\"4j1b\", selections.all(\"exactly_1l\", \"atleast_4j\", \"exactly_1b\"))\n",
    "            selections.add(\"4j2b\", selections.all(\"exactly_1l\", \"atleast_4j\", \"atleast_2b\"))\n",
    "\n",
    "            for region in [\"4j1b\", \"4j2b\"]:\n",
    "                region_selection = selections.all(region)\n",
    "                region_jets = jets[region_selection]\n",
    "                region_elecs = elecs[region_selection]\n",
    "                region_muons = muons[region_selection]\n",
    "                region_weights = np.ones(len(region_jets)) * xsec_weight\n",
    "                if self.use_inference:\n",
    "                    region_even = even[region_selection]\n",
    "\n",
    "                if region == \"4j1b\":\n",
    "                    observable = ak.sum(region_jets.pt, axis=-1)\n",
    "\n",
    "                elif region == \"4j2b\":\n",
    "\n",
    "                    # reconstruct hadronic top as bjj system with largest pT\n",
    "                    trijet = ak.combinations(region_jets, 3, fields=[\"j1\", \"j2\", \"j3\"])  # trijet candidates\n",
    "                    trijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3  # calculate four-momentum of tri-jet system\n",
    "                    trijet[\"max_btag\"] = np.maximum(trijet.j1.btagCSVV2, np.maximum(trijet.j2.btagCSVV2, trijet.j3.btagCSVV2))\n",
    "                    trijet = trijet[trijet.max_btag > B_TAG_THRESHOLD]  # at least one-btag in trijet candidates\n",
    "                    # pick trijet candidate with largest pT and calculate mass of system\n",
    "                    trijet_mass = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].mass\n",
    "                    observable = ak.flatten(trijet_mass)\n",
    "\n",
    "                    if sum(region_selection)==0:\n",
    "                        continue\n",
    "\n",
    "                    if self.use_inference:\n",
    "                        features, perm_counts = utils.ml.get_features(\n",
    "                            region_jets,\n",
    "                            region_elecs,\n",
    "                            region_muons,\n",
    "                            max_n_jets=utils.config[\"ml\"][\"MAX_N_JETS\"],\n",
    "                        )\n",
    "                        even_perm = np.repeat(region_even, perm_counts)\n",
    "\n",
    "                        # calculate ml observable\n",
    "                        if self.use_triton:\n",
    "                            results = utils.ml.get_inference_results_triton(\n",
    "                                features,\n",
    "                                even_perm,\n",
    "                                triton_client,\n",
    "                                utils.config[\"ml\"][\"MODEL_NAME\"],\n",
    "                                utils.config[\"ml\"][\"MODEL_VERSION_EVEN\"],\n",
    "                                utils.config[\"ml\"][\"MODEL_VERSION_ODD\"],\n",
    "                            )\n",
    "\n",
    "                        else:\n",
    "                            results = utils.ml.get_inference_results_local(\n",
    "                                features,\n",
    "                                even_perm,\n",
    "                                utils.ml.model_even,\n",
    "                                utils.ml.model_odd,\n",
    "                            )\n",
    "                            \n",
    "                        results = ak.unflatten(results, perm_counts)\n",
    "                        features = ak.flatten(ak.unflatten(features, perm_counts)[\n",
    "                            ak.from_regular(ak.argmax(results,axis=1)[:, np.newaxis])\n",
    "                        ])\n",
    "                syst_var_name = f\"{syst_var}\"\n",
    "                # Break up the filling into event weight systematics and object variation systematics\n",
    "                if syst_var in event_systs:\n",
    "                    for i_dir, direction in enumerate([\"up\", \"down\"]):\n",
    "                        # Should be an event weight systematic with an up/down variation\n",
    "                        if syst_var.startswith(\"btag_var\"):\n",
    "                            i_jet = int(syst_var.rsplit(\"_\",1)[-1])   # Kind of fragile\n",
    "                            wgt_variation = self.cset[\"event_systematics\"].evaluate(\"btag_var\", direction, region_jets.pt[:,i_jet])\n",
    "                        elif syst_var == \"scale_var\":\n",
    "                            # The pt array is only used to make sure the output array has the correct shape\n",
    "                            wgt_variation = self.cset[\"event_systematics\"].evaluate(\"scale_var\", direction, region_jets.pt[:,0])\n",
    "                        syst_var_name = f\"{syst_var}_{direction}\"\n",
    "                        hist_dict[region].fill(\n",
    "                            observable=observable, process=process,\n",
    "                            variation=syst_var_name, weight=region_weights * wgt_variation\n",
    "                        )\n",
    "                        if region == \"4j2b\" and self.use_inference:\n",
    "                            for i in range(len(utils.config[\"ml\"][\"FEATURE_NAMES\"])):\n",
    "                                ml_hist_dict[utils.config[\"ml\"][\"FEATURE_NAMES\"][i]].fill(\n",
    "                                    observable=features[..., i], process=process,\n",
    "                                    variation=syst_var_name, weight=region_weights * wgt_variation\n",
    "                                )\n",
    "                else:\n",
    "                    # Should either be 'nominal' or an object variation systematic\n",
    "                    if variation != \"nominal\":\n",
    "                        # This is a 2-point systematic, e.g. ttbar__scaledown, ttbar__ME_var, etc.\n",
    "                        syst_var_name = variation\n",
    "                    hist_dict[region].fill(\n",
    "                        observable=observable, process=process,\n",
    "                        variation=syst_var_name, weight=region_weights\n",
    "                    )\n",
    "                    if region == \"4j2b\" and self.use_inference:\n",
    "                        for i in range(len(utils.config[\"ml\"][\"FEATURE_NAMES\"])):\n",
    "                            ml_hist_dict[utils.config[\"ml\"][\"FEATURE_NAMES\"][i]].fill(\n",
    "                                observable=features[..., i], process=process,\n",
    "                                variation=syst_var_name, weight=region_weights\n",
    "                            )\n",
    "\n",
    "\n",
    "        output = {\"nevents\": {events.metadata[\"dataset\"]: len(events)}, \"hist_dict\": hist_dict}\n",
    "        if self.use_inference:\n",
    "            output[\"ml_hist_dict\"] = ml_hist_dict\n",
    "\n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileset = utils.file_input.construct_fileset(\n",
    "    N_FILES_MAX_PER_SAMPLE,\n",
    "    use_xcache=False,\n",
    "    af_name=utils.config[\"benchmarking\"][\"AF_NAME\"],  # local files on /data for af_name=\"ssl-dev\"\n",
    "    input_from_eos=utils.config[\"benchmarking\"][\"INPUT_FROM_EOS\"],\n",
    "    xcache_atlas_prefix=utils.config[\"benchmarking\"][\"XCACHE_ATLAS_PREFIX\"],\n",
    ")\n",
    "\n",
    "print(f\"processes in fileset: {list(fileset.keys())}\")\n",
    "print(f\"\\nexample of information in fileset:\\n{{\\n  'files': [{fileset['ttbar__nominal']['files'][0]}, ...],\")\n",
    "print(f\"  'metadata': {fileset['ttbar__nominal']['metadata']}\\n}}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query(source):\n",
    "    \"\"\"Query for event / column selection: >=4j >=1b, ==1 lep with pT>30 GeV + additional cuts,\n",
    "    return relevant columns\n",
    "    *NOTE* jet pT cut is set lower to account for systematic variations to jet pT\n",
    "    \"\"\"\n",
    "    cuts = source.Where(lambda e: {\"pt\": e.Electron_pt,\n",
    "                               \"eta\": e.Electron_eta,\n",
    "                               \"cutBased\": e.Electron_cutBased,\n",
    "                               \"sip3d\": e.Electron_sip3d,}.Zip()\\\n",
    "                        .Where(lambda electron: (electron.pt > 30\n",
    "                                                 and abs(electron.eta) < 2.1\n",
    "                                                 and electron.cutBased == 4\n",
    "                                                 and electron.sip3d < 4)).Count()\n",
    "                        + {\"pt\": e.Muon_pt,\n",
    "                           \"eta\": e.Muon_eta,\n",
    "                           \"tightId\": e.Muon_tightId,\n",
    "                           \"sip3d\": e.Muon_sip3d,\n",
    "                           \"pfRelIso04_all\": e.Muon_pfRelIso04_all}.Zip()\\\n",
    "                        .Where(lambda muon: (muon.pt > 30\n",
    "                                             and abs(muon.eta) < 2.1\n",
    "                                             and muon.tightId\n",
    "                                             and muon.pfRelIso04_all < 0.15)).Count()== 1)\\\n",
    "                        .Where(lambda f: {\"pt\": f.Jet_pt,\n",
    "                                          \"eta\": f.Jet_eta,\n",
    "                                          \"jetId\": f.Jet_jetId}.Zip()\\\n",
    "                               .Where(lambda jet: (jet.pt > 25\n",
    "                                                   and abs(jet.eta) < 2.4\n",
    "                                                   and jet.jetId == 6)).Count() >= 4)\\\n",
    "                        .Where(lambda g: {\"pt\": g.Jet_pt,\n",
    "                                          \"eta\": g.Jet_eta,\n",
    "                                          \"btagCSVV2\": g.Jet_btagCSVV2,\n",
    "                                          \"jetId\": g.Jet_jetId}.Zip()\\\n",
    "                        .Where(lambda jet: (jet.btagCSVV2 > 0.5\n",
    "                                            and jet.pt > 25\n",
    "                                            and abs(jet.eta) < 2.4)\n",
    "                                            and jet.jetId == 6).Count() >= 1)\n",
    "    selection = cuts.Select(lambda h: {\"Electron_pt\": h.Electron_pt,\n",
    "                                       \"Electron_eta\": h.Electron_eta,\n",
    "                                       \"Electron_phi\": h.Electron_phi,\n",
    "                                       \"Electron_mass\": h.Electron_mass,\n",
    "                                       \"Electron_cutBased\": h.Electron_cutBased,\n",
    "                                       \"Electron_sip3d\": h.Electron_sip3d,\n",
    "                                       \"Muon_pt\": h.Muon_pt,\n",
    "                                       \"Muon_eta\": h.Muon_eta,\n",
    "                                       \"Muon_phi\": h.Muon_phi,\n",
    "                                       \"Muon_mass\": h.Muon_mass,\n",
    "                                       \"Muon_tightId\": h.Muon_tightId,\n",
    "                                       \"Muon_sip3d\": h.Muon_sip3d,\n",
    "                                       \"Muon_pfRelIso04_all\": h.Muon_pfRelIso04_all,\n",
    "                                       \"Jet_mass\": h.Jet_mass,\n",
    "                                       \"Jet_pt\": h.Jet_pt,\n",
    "                                       \"Jet_eta\": h.Jet_eta,\n",
    "                                       \"Jet_phi\": h.Jet_phi,\n",
    "                                       \"Jet_qgl\": h.Jet_qgl,\n",
    "                                       \"Jet_btagCSVV2\": h.Jet_btagCSVV2,\n",
    "                                       \"Jet_jetId\": h.Jet_jetId,\n",
    "                                       \"event\": h.event,\n",
    "                                      })\n",
    "    if USE_INFERENCE:\n",
    "        return selection\n",
    "\n",
    "    # some branches are only needed if USE_INFERENCE is turned on\n",
    "    return selection.Select(lambda h: {\"Electron_pt\": h.Electron_pt,\n",
    "                                       \"Electron_eta\": h.Electron_eta,\n",
    "                                       \"Electron_cutBased\": h.Electron_cutBased,\n",
    "                                       \"Electron_sip3d\": h.Electron_sip3d,\n",
    "                                       \"Muon_pt\": h.Muon_pt,\n",
    "                                       \"Muon_eta\": h.Muon_eta,\n",
    "                                       \"Muon_tightId\": h.Muon_tightId,\n",
    "                                       \"Muon_sip3d\": h.Muon_sip3d,\n",
    "                                       \"Muon_pfRelIso04_all\": h.Muon_pfRelIso04_all,\n",
    "                                       \"Jet_mass\": h.Jet_mass,\n",
    "                                       \"Jet_pt\": h.Jet_pt,\n",
    "                                       \"Jet_eta\": h.Jet_eta,\n",
    "                                       \"Jet_phi\": h.Jet_phi,\n",
    "                                       \"Jet_btagCSVV2\": h.Jet_btagCSVV2,\n",
    "                                       \"Jet_jetId\": h.Jet_jetId,\n",
    "                                      })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import func_adl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SERVICEX:\n",
    "    try:\n",
    "        from func_adl_servicex import ServiceXSourceUpROOT\n",
    "    except ImportError:\n",
    "        print(\"cannot import func_adl_servicex, which is a required dependency when using ServiceX\")\n",
    "        raise\n",
    "\n",
    "    # dummy dataset on which to generate the query\n",
    "    dummy_ds = ServiceXSourceUpROOT(\"cernopendata://dummy\", \"Events\", backend_name=\"uproot\")\n",
    "\n",
    "    # tell low-level infrastructure not to contact ServiceX yet, only to\n",
    "    # return the qastle string it would have sent\n",
    "    dummy_ds.return_qastle = True\n",
    "\n",
    "    # create the query\n",
    "    query = get_query(dummy_ds).value()\n",
    "\n",
    "    # now we query the files using a wrapper around ServiceXDataset to transform all processes at once\n",
    "    t0 = time.time()\n",
    "    ds = utils.file_input.ServiceXDatasetGroup(fileset, backend_name=\"uproot\", ignore_cache=utils.config[\"global\"][\"SERVICEX_IGNORE_CACHE\"])\n",
    "    files_per_process = ds.get_data_rootfiles_uri(query, as_signed_url=True, title=\"CMS ttbar\")\n",
    "\n",
    "    print(f\"ServiceX data delivery took {time.time() - t0:.2f} seconds\")\n",
    "\n",
    "    # update fileset to point to ServiceX-transformed files\n",
    "    for process in fileset.keys():\n",
    "        fileset[process][\"files\"] = [f.url for f in files_per_process[process]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "structured_fileset = {}\n",
    "\n",
    "for key, value in fileset.items():\n",
    "    process = value['metadata']['process']\n",
    "    variation = value['metadata']['variation']\n",
    "    nevts = value['metadata']['nevts']\n",
    "    \n",
    "    if process not in structured_fileset:\n",
    "        structured_fileset[process] = {}\n",
    "    \n",
    "    if variation not in structured_fileset[process]:\n",
    "        structured_fileset[process][variation] = {\n",
    "            'nevts_total': nevts,\n",
    "            'files': []\n",
    "        }\n",
    "    \n",
    "    for file_path in value['files']:\n",
    "        structured_fileset[process][variation]['files'].append({\n",
    "            'path': file_path,\n",
    "            'nevts': nevts\n",
    "        })\n",
    "\n",
    "# Write the structured dictionary to a JSON file\n",
    "output_file = 'file_inputs_servicex.json'\n",
    "with open(output_file, 'w') as json_file:\n",
    "    json.dump(structured_fileset, json_file, indent=4)\n",
    "\n",
    "print(f\"Structured fileset has been written to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
